<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>torch_mist.utils.data.loader &mdash; torch_mist  documentation</title>
      <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
        <script src="../../../../../_static/sphinx_highlight.js"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <link rel="next" title="torch_mist.utils.data.sampler" href="../sampler/index.html" />
    <link rel="prev" title="torch_mist.utils.data.dataset" href="../dataset/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #1a1a1a" >

          
          
          <a href="../../../../../index.html">
            
              <img src="../../../../../_static/logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../../example.html">Basic Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../example.html#advanced-usage">Advanced Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../conduct.html">Code of Conduct</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../../index.html">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist</span></code></a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../../index.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../../../critic/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist.critic</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../data/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist.data</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../distributions/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist.distributions</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../estimators/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist.estimators</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nn/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist.nn</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../quantization/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist.quantization</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../train/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist.train</span></code></a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist.utils</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../index.html#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../index.html#package-contents">Package Contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #1a1a1a" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">torch_mist</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../../index.html">API Reference</a></li>
          <li class="breadcrumb-item"><a href="../../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist</span></code></a></li>
          <li class="breadcrumb-item"><a href="../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist.utils</span></code></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist.utils.data</span></code></a></li>
      <li class="breadcrumb-item active"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist.utils.data.loader</span></code></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../_sources/autoapi/torch_mist/utils/data/loader/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-torch_mist.utils.data.loader">
<span id="torch-mist-utils-data-loader"></span><h1><a class="reference internal" href="#module-torch_mist.utils.data.loader" title="torch_mist.utils.data.loader"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch_mist.utils.data.loader</span></code></a><a class="headerlink" href="#module-torch_mist.utils.data.loader" title="Permalink to this heading"></a></h1>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this heading"></a></h2>
<section id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this heading"></a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#torch_mist.utils.data.loader.DistributionDataLoader" title="torch_mist.utils.data.loader.DistributionDataLoader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DistributionDataLoader</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#torch_mist.utils.data.loader.SameAttributeDataLoader" title="torch_mist.utils.data.loader.SameAttributeDataLoader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SameAttributeDataLoader</span></code></a></p></td>
<td><p>Data loader. Combines a dataset and a sampler, and provides an iterable over</p></td>
</tr>
</tbody>
</table>
</section>
<section id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this heading"></a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#torch_mist.utils.data.loader.sample_same_attributes" title="torch_mist.utils.data.loader.sample_same_attributes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sample_same_attributes</span></code></a>(dataloader, attributes)</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt class="sig sig-object py" id="torch_mist.utils.data.loader.DistributionDataLoader">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch_mist.utils.data.loader.</span></span><span class="sig-name descname"><span class="pre">DistributionDataLoader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_dist</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.distributions.Distribution</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../../distributions/joint/index.html#torch_mist.distributions.joint.JointDistribution" title="torch_mist.distributions.joint.JointDistribution"><span class="pre">torch_mist.distributions.joint.JointDistribution</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../_modules/torch_mist/utils/data/loader.html#DistributionDataLoader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_mist.utils.data.loader.DistributionDataLoader" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">collections.Iterator</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>]]</p>
<dl class="py method">
<dt class="sig sig-object py" id="torch_mist.utils.data.loader.DistributionDataLoader.__next__">
<span class="sig-name descname"><span class="pre">__next__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../../../_modules/torch_mist/utils/data/loader.html#DistributionDataLoader.__next__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_mist.utils.data.loader.DistributionDataLoader.__next__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch_mist.utils.data.loader.DistributionDataLoader.__iter__">
<span class="sig-name descname"><span class="pre">__iter__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../../../_modules/torch_mist/utils/data/loader.html#DistributionDataLoader.__iter__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_mist.utils.data.loader.DistributionDataLoader.__iter__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch_mist.utils.data.loader.DistributionDataLoader.__len__">
<span class="sig-name descname"><span class="pre">__len__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../../../_modules/torch_mist/utils/data/loader.html#DistributionDataLoader.__len__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_mist.utils.data.loader.DistributionDataLoader.__len__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch_mist.utils.data.loader.SameAttributeDataLoader">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch_mist.utils.data.loader.</span></span><span class="sig-name descname"><span class="pre">SameAttributeDataLoader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.utils.data.Dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attributes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">numpy.ndarray</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../_modules/torch_mist/utils/data/loader.html#SameAttributeDataLoader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_mist.utils.data.loader.SameAttributeDataLoader" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></p>
<p>Data loader. Combines a dataset and a sampler, and provides an iterable over
the given dataset.</p>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> supports both map-style and
iterable-style datasets with single- or multi-process loading, customizing
loading order and optional automatic batching (collation) and memory pinning.</p>
<p>See <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.utils.data</span></code> documentation page for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<em>Dataset</em>) – dataset from which to load the data.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – how many samples per batch to load
(default: <code class="docutils literal notranslate"><span class="pre">1</span></code>).</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em><em>, </em><em>optional</em>) – set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to have the data reshuffled
at every epoch (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>).</p></li>
<li><p><strong>sampler</strong> (<em>Sampler</em><em> or </em><em>Iterable</em><em>, </em><em>optional</em>) – defines the strategy to draw
samples from the dataset. Can be any <code class="docutils literal notranslate"><span class="pre">Iterable</span></code> with <code class="docutils literal notranslate"><span class="pre">__len__</span></code>
implemented. If specified, <code class="xref py py-attr docutils literal notranslate"><span class="pre">shuffle</span></code> must not be specified.</p></li>
<li><p><strong>batch_sampler</strong> (<em>Sampler</em><em> or </em><em>Iterable</em><em>, </em><em>optional</em>) – like <code class="xref py py-attr docutils literal notranslate"><span class="pre">sampler</span></code>, but
returns a batch of indices at a time. Mutually exclusive with
<code class="xref py py-attr docutils literal notranslate"><span class="pre">batch_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">shuffle</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">sampler</span></code>,
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">drop_last</span></code>.</p></li>
<li><p><strong>num_workers</strong> (<em>int</em><em>, </em><em>optional</em>) – how many subprocesses to use for data
loading. <code class="docutils literal notranslate"><span class="pre">0</span></code> means that the data will be loaded in the main process.
(default: <code class="docutils literal notranslate"><span class="pre">0</span></code>)</p></li>
<li><p><strong>collate_fn</strong> (<em>Callable</em><em>, </em><em>optional</em>) – merges a list of samples to form a
mini-batch of Tensor(s).  Used when using batched loading from a
map-style dataset.</p></li>
<li><p><strong>pin_memory</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the data loader will copy Tensors
into device/CUDA pinned memory before returning them.  If your data elements
are a custom type, or your <code class="xref py py-attr docutils literal notranslate"><span class="pre">collate_fn</span></code> returns a batch that is a custom type,
see the example below.</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em><em>, </em><em>optional</em>) – set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to drop the last incomplete batch,
if the dataset size is not divisible by the batch size. If <code class="docutils literal notranslate"><span class="pre">False</span></code> and
the size of dataset is not divisible by the batch size, then the last batch
will be smaller. (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p><strong>timeout</strong> (<em>numeric</em><em>, </em><em>optional</em>) – if positive, the timeout value for collecting a batch
from workers. Should always be non-negative. (default: <code class="docutils literal notranslate"><span class="pre">0</span></code>)</p></li>
<li><p><strong>worker_init_fn</strong> (<em>Callable</em><em>, </em><em>optional</em>) – If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, this will be called on each
worker subprocess with the worker id (an int in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">num_workers</span> <span class="pre">-</span> <span class="pre">1]</span></code>) as
input, after seeding and before data loading. (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>generator</strong> (<em>torch.Generator</em><em>, </em><em>optional</em>) – If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, this RNG will be used
by RandomSampler to generate random indexes and multiprocessing to generate
<cite>base_seed</cite> for workers. (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>prefetch_factor</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>keyword-only arg</em>) – Number of batches loaded
in advance by each worker. <code class="docutils literal notranslate"><span class="pre">2</span></code> means there will be a total of
2 * num_workers batches prefetched across all workers. (default value depends
on the set value for num_workers. If value of num_workers=0 default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.
Otherwise if value of num_workers&gt;0 default is <code class="docutils literal notranslate"><span class="pre">2</span></code>).</p></li>
<li><p><strong>persistent_workers</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the data loader will not shutdown
the worker processes after a dataset has been consumed once. This allows to
maintain the workers <cite>Dataset</cite> instances alive. (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p><strong>pin_memory_device</strong> (<em>str</em><em>, </em><em>optional</em>) – the data loader will copy Tensors
into device pinned memory before returning them if pin_memory is set to true.</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">spawn</span></code> start method is used, <code class="xref py py-attr docutils literal notranslate"><span class="pre">worker_init_fn</span></code>
cannot be an unpicklable object, e.g., a lambda function. See
<span class="xref std std-ref">multiprocessing-best-practices</span> on more details related
to multiprocessing in PyTorch.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">len(dataloader)</span></code> heuristic is based on the length of the sampler used.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">dataset</span></code> is an <code class="xref py py-class docutils literal notranslate"><span class="pre">IterableDataset</span></code>,
it instead returns an estimate based on <code class="docutils literal notranslate"><span class="pre">len(dataset)</span> <span class="pre">/</span> <span class="pre">batch_size</span></code>, with proper
rounding depending on <code class="xref py py-attr docutils literal notranslate"><span class="pre">drop_last</span></code>, regardless of multi-process loading
configurations. This represents the best guess PyTorch can make because PyTorch
trusts user <code class="xref py py-attr docutils literal notranslate"><span class="pre">dataset</span></code> code in correctly handling multi-process
loading to avoid duplicate data.</p>
<p>However, if sharding results in multiple workers having incomplete last batches,
this estimate can still be inaccurate, because (1) an otherwise complete batch can
be broken into multiple ones and (2) more than one batch worth of samples can be
dropped when <code class="xref py py-attr docutils literal notranslate"><span class="pre">drop_last</span></code> is set. Unfortunately, PyTorch can not detect such
cases in general.</p>
<p>See <a href="#id1"><span class="problematic" id="id2">`Dataset Types`_</span></a> for more details on these two types of datasets and how
<code class="xref py py-class docutils literal notranslate"><span class="pre">IterableDataset</span></code> interacts with
<a href="#id3"><span class="problematic" id="id4">`Multi-process data loading`_</span></a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>See <span class="xref std std-ref">reproducibility</span>, and <span class="xref std std-ref">dataloader-workers-random-seed</span>, and
<span class="xref std std-ref">data-loading-randomness</span> notes for random seed related questions.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_mist.utils.data.loader.sample_same_attributes">
<span class="sig-prename descclassname"><span class="pre">torch_mist.utils.data.loader.</span></span><span class="sig-name descname"><span class="pre">sample_same_attributes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataloader</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.utils.data.DataLoader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attributes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">numpy.ndarray</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../_modules/torch_mist/utils/data/loader.html#sample_same_attributes"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_mist.utils.data.loader.sample_same_attributes" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../dataset/index.html" class="btn btn-neutral float-left" title="torch_mist.utils.data.dataset" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../sampler/index.html" class="btn btn-neutral float-right" title="torch_mist.utils.data.sampler" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Marco Federici.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>