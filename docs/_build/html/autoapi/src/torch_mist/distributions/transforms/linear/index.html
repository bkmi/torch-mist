<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>src.torch_mist.distributions.transforms.linear &mdash; torch_mist  documentation</title>
      <link rel="stylesheet" href="../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../../../" id="documentation_options" src="../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../../../_static/doctools.js"></script>
        <script src="../../../../../../_static/sphinx_highlight.js"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" />
    <link rel="next" title="src.torch_mist.distributions.transforms.permute" href="../permute/index.html" />
    <link rel="prev" title="src.torch_mist.distributions.transforms.base" href="../base/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            torch_mist
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../../../example.html">Example usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../conduct.html">Code of Conduct</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../../../index.html">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">src</span></code></a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../../../index.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">src.torch_mist</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">torch_mist</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../../../index.html">API Reference</a></li>
          <li class="breadcrumb-item"><a href="../../../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">src</span></code></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">src.torch_mist</span></code></a></li>
          <li class="breadcrumb-item"><a href="../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">src.torch_mist.distributions</span></code></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">src.torch_mist.distributions.transforms</span></code></a></li>
      <li class="breadcrumb-item active"><code class="xref py py-mod docutils literal notranslate"><span class="pre">src.torch_mist.distributions.transforms.linear</span></code></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/autoapi/src/torch_mist/distributions/transforms/linear/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-src.torch_mist.distributions.transforms.linear">
<span id="src-torch-mist-distributions-transforms-linear"></span><h1><a class="reference internal" href="#module-src.torch_mist.distributions.transforms.linear" title="src.torch_mist.distributions.transforms.linear"><code class="xref py py-mod docutils literal notranslate"><span class="pre">src.torch_mist.distributions.transforms.linear</span></code></a><a class="headerlink" href="#module-src.torch_mist.distributions.transforms.linear" title="Permalink to this heading"></a></h1>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this heading"></a></h2>
<section id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this heading"></a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear" title="src.torch_mist.distributions.transforms.linear.ConditionedLinear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConditionedLinear</span></code></a></p></td>
<td><p>Abstract class for invertable transformations with computable log</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#src.torch_mist.distributions.transforms.linear.Linear" title="src.torch_mist.distributions.transforms.linear.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>Abstract class for invertable transformations with computable log</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#src.torch_mist.distributions.transforms.linear.ConditionalLinear" title="src.torch_mist.distributions.transforms.linear.ConditionalLinear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConditionalLinear</span></code></a></p></td>
<td><p>Conditional transforms with learnable parameters such as normalizing flows should inherit from this class rather</p></td>
</tr>
</tbody>
</table>
</section>
<section id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this heading"></a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#src.torch_mist.distributions.transforms.linear.linear" title="src.torch_mist.distributions.transforms.linear.linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">linear</span></code></a>(input_dim[, loc, scale, initial_scale])</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#src.torch_mist.distributions.transforms.linear.conditional_linear" title="src.torch_mist.distributions.transforms.linear.conditional_linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conditional_linear</span></code></a>(input_dim, context_dim[, ...])</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#src.torch_mist.distributions.transforms.linear.conditional_skip_linear" title="src.torch_mist.distributions.transforms.linear.conditional_skip_linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conditional_skip_linear</span></code></a>(input_dim, context_dim[, ...])</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionedLinear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.torch_mist.distributions.transforms.linear.</span></span><span class="sig-name descname"><span class="pre">ConditionedLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.distributions.Transform</span></code></p>
<p>Abstract class for invertable transformations with computable log
det jacobians. They are primarily used in
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.TransformedDistribution</span></code>.</p>
<p>Caching is useful for transforms whose inverses are either expensive or
numerically unstable. Note that care must be taken with memoized values
since the autograd graph may be reversed. For example while the following
works with or without caching:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">t</span><span class="o">.</span><span class="n">log_abs_det_jacobian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># x will receive gradients.</span>
</pre></div>
</div>
<p>However the following will error when caching due to dependency reversal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">grad</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="p">[</span><span class="n">y</span><span class="p">])</span>  <span class="c1"># error because z is x</span>
</pre></div>
</div>
<p>Derived classes should implement one or both of <a class="reference internal" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear._call" title="src.torch_mist.distributions.transforms.linear.ConditionedLinear._call"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_call()</span></code></a> or
<a class="reference internal" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear._inverse" title="src.torch_mist.distributions.transforms.linear.ConditionedLinear._inverse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_inverse()</span></code></a>. Derived classes that set <cite>bijective=True</cite> should also
implement <a class="reference internal" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear.log_abs_det_jacobian" title="src.torch_mist.distributions.transforms.linear.ConditionedLinear.log_abs_det_jacobian"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log_abs_det_jacobian()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>cache_size</strong> (<em>int</em>) – Size of cache. If zero, no caching is done. If one,
the latest single value is cached. Only 0 and 1 are supported.</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionedLinear.domain">
<span class="sig-name descname"><span class="pre">domain</span></span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear.domain" title="Permalink to this definition"></a></dt>
<dd><p>The constraint representing valid inputs to this transform.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionedLinear.codomain">
<span class="sig-name descname"><span class="pre">codomain</span></span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear.codomain" title="Permalink to this definition"></a></dt>
<dd><p>The constraint representing valid outputs to this transform
which are inputs to the inverse transform.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionedLinear.bijective">
<span class="sig-name descname"><span class="pre">bijective</span></span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear.bijective" title="Permalink to this definition"></a></dt>
<dd><p>Whether this transform is bijective. A transform
<code class="docutils literal notranslate"><span class="pre">t</span></code> is bijective iff <code class="docutils literal notranslate"><span class="pre">t.inv(t(x))</span> <span class="pre">==</span> <span class="pre">x</span></code> and
<code class="docutils literal notranslate"><span class="pre">t(t.inv(y))</span> <span class="pre">==</span> <span class="pre">y</span></code> for every <code class="docutils literal notranslate"><span class="pre">x</span></code> in the domain and <code class="docutils literal notranslate"><span class="pre">y</span></code> in
the codomain. Transforms that are not bijective should at least
maintain the weaker pseudoinverse properties
<code class="docutils literal notranslate"><span class="pre">t(t.inv(t(x))</span> <span class="pre">==</span> <span class="pre">t(x)</span></code> and <code class="docutils literal notranslate"><span class="pre">t.inv(t(t.inv(y)))</span> <span class="pre">==</span> <span class="pre">t.inv(y)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionedLinear.sign">
<span class="sig-name descname"><span class="pre">sign</span></span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear.sign" title="Permalink to this definition"></a></dt>
<dd><p>For bijective univariate transforms, this
should be +1 or -1 depending on whether transform is monotone
increasing or decreasing.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int or Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id0">
<span class="sig-name descname"><span class="pre">domain</span></span><a class="headerlink" href="#id0" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id1">
<span class="sig-name descname"><span class="pre">codomain</span></span><a class="headerlink" href="#id1" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id2">
<span class="sig-name descname"><span class="pre">bijective</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#id2" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionedLinear._call">
<span class="sig-name descname"><span class="pre">_call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear._call" title="Permalink to this definition"></a></dt>
<dd><p>Abstract method to compute forward transformation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionedLinear._inverse">
<span class="sig-name descname"><span class="pre">_inverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear._inverse" title="Permalink to this definition"></a></dt>
<dd><p>Abstract method to compute inverse transformation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionedLinear.log_abs_det_jacobian">
<span class="sig-name descname"><span class="pre">log_abs_det_jacobian</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear.log_abs_det_jacobian" title="Permalink to this definition"></a></dt>
<dd><p>Computes the log det jacobian <cite>log |dy/dx|</cite> given input and output.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.Linear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.torch_mist.distributions.transforms.linear.</span></span><span class="sig-name descname"><span class="pre">Linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.Linear" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#src.torch_mist.distributions.transforms.linear.ConditionedLinear" title="src.torch_mist.distributions.transforms.linear.ConditionedLinear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConditionedLinear</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">pyro.distributions.TransformModule</span></code></p>
<p>Abstract class for invertable transformations with computable log
det jacobians. They are primarily used in
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.TransformedDistribution</span></code>.</p>
<p>Caching is useful for transforms whose inverses are either expensive or
numerically unstable. Note that care must be taken with memoized values
since the autograd graph may be reversed. For example while the following
works with or without caching:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">t</span><span class="o">.</span><span class="n">log_abs_det_jacobian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># x will receive gradients.</span>
</pre></div>
</div>
<p>However the following will error when caching due to dependency reversal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">grad</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="p">[</span><span class="n">y</span><span class="p">])</span>  <span class="c1"># error because z is x</span>
</pre></div>
</div>
<p>Derived classes should implement one or both of <code class="xref py py-meth docutils literal notranslate"><span class="pre">_call()</span></code> or
<code class="xref py py-meth docutils literal notranslate"><span class="pre">_inverse()</span></code>. Derived classes that set <cite>bijective=True</cite> should also
implement <code class="xref py py-meth docutils literal notranslate"><span class="pre">log_abs_det_jacobian()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>cache_size</strong> (<em>int</em>) – Size of cache. If zero, no caching is done. If one,
the latest single value is cached. Only 0 and 1 are supported.</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.Linear.domain">
<span class="sig-name descname"><span class="pre">domain</span></span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.Linear.domain" title="Permalink to this definition"></a></dt>
<dd><p>The constraint representing valid inputs to this transform.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.Linear.codomain">
<span class="sig-name descname"><span class="pre">codomain</span></span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.Linear.codomain" title="Permalink to this definition"></a></dt>
<dd><p>The constraint representing valid outputs to this transform
which are inputs to the inverse transform.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.Linear.bijective">
<span class="sig-name descname"><span class="pre">bijective</span></span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.Linear.bijective" title="Permalink to this definition"></a></dt>
<dd><p>Whether this transform is bijective. A transform
<code class="docutils literal notranslate"><span class="pre">t</span></code> is bijective iff <code class="docutils literal notranslate"><span class="pre">t.inv(t(x))</span> <span class="pre">==</span> <span class="pre">x</span></code> and
<code class="docutils literal notranslate"><span class="pre">t(t.inv(y))</span> <span class="pre">==</span> <span class="pre">y</span></code> for every <code class="docutils literal notranslate"><span class="pre">x</span></code> in the domain and <code class="docutils literal notranslate"><span class="pre">y</span></code> in
the codomain. Transforms that are not bijective should at least
maintain the weaker pseudoinverse properties
<code class="docutils literal notranslate"><span class="pre">t(t.inv(t(x))</span> <span class="pre">==</span> <span class="pre">t(x)</span></code> and <code class="docutils literal notranslate"><span class="pre">t.inv(t(t.inv(y)))</span> <span class="pre">==</span> <span class="pre">t.inv(y)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.Linear.sign">
<span class="sig-name descname"><span class="pre">sign</span></span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.Linear.sign" title="Permalink to this definition"></a></dt>
<dd><p>For bijective univariate transforms, this
should be +1 or -1 depending on whether transform is monotone
increasing or decreasing.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int or Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.Linear._params">
<span class="sig-name descname"><span class="pre">_params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.Linear._params" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionalLinear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.torch_mist.distributions.transforms.linear.</span></span><span class="sig-name descname"><span class="pre">ConditionalLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">net</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_connection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionalLinear" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">pyro.distributions.ConditionalTransformModule</span></code></p>
<p>Conditional transforms with learnable parameters such as normalizing flows should inherit from this class rather
than <code class="xref py py-class docutils literal notranslate"><span class="pre">ConditionalTransform</span></code> so they are also a subclass of
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> and inherit all the useful methods of that class.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionalLinear.domain">
<span class="sig-name descname"><span class="pre">domain</span></span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionalLinear.domain" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionalLinear.codomain">
<span class="sig-name descname"><span class="pre">codomain</span></span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionalLinear.codomain" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionalLinear.bijective">
<span class="sig-name descname"><span class="pre">bijective</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionalLinear.bijective" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionalLinear._params">
<span class="sig-name descname"><span class="pre">_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">context</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionalLinear._params" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.ConditionalLinear.condition">
<span class="sig-name descname"><span class="pre">condition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">context</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.ConditionalLinear.condition" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.distributions.Transform</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.linear">
<span class="sig-prename descclassname"><span class="pre">src.torch_mist.distributions.transforms.linear.</span></span><span class="sig-name descname"><span class="pre">linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.linear" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.conditional_linear">
<span class="sig-prename descclassname"><span class="pre">src.torch_mist.distributions.transforms.linear.</span></span><span class="sig-name descname"><span class="pre">conditional_linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.conditional_linear" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="src.torch_mist.distributions.transforms.linear.conditional_skip_linear">
<span class="sig-prename descclassname"><span class="pre">src.torch_mist.distributions.transforms.linear.</span></span><span class="sig-name descname"><span class="pre">conditional_skip_linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learn_loc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.torch_mist.distributions.transforms.linear.conditional_skip_linear" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../base/index.html" class="btn btn-neutral float-left" title="src.torch_mist.distributions.transforms.base" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../permute/index.html" class="btn btn-neutral float-right" title="src.torch_mist.distributions.transforms.permute" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Marco Federici.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>