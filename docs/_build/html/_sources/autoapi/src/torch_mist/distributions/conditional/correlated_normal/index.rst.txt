:py:mod:`src.torch_mist.distributions.conditional.correlated_normal`
====================================================================

.. py:module:: src.torch_mist.distributions.conditional.correlated_normal


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   src.torch_mist.distributions.conditional.correlated_normal.CorrelatedNormal
   src.torch_mist.distributions.conditional.correlated_normal.CubicPTransform
   src.torch_mist.distributions.conditional.correlated_normal.RandomFlipTransform
   src.torch_mist.distributions.conditional.correlated_normal.CubicCorrelatedNormal
   src.torch_mist.distributions.conditional.correlated_normal.Translate
   src.torch_mist.distributions.conditional.correlated_normal.SkewedCorrelatedNormal




.. py:class:: CorrelatedNormal(rho: float)


   Bases: :py:obj:`pyro.distributions.ConditionalDistribution`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:method:: condition(context)

      :rtype: torch.distributions.Distribution



.. py:class:: CubicPTransform(k1: float = 1.0, k3: float = 1.0)


   Bases: :py:obj:`torch.distributions.Transform`

   Abstract class for invertable transformations with computable log
   det jacobians. They are primarily used in
   :class:`torch.distributions.TransformedDistribution`.

   Caching is useful for transforms whose inverses are either expensive or
   numerically unstable. Note that care must be taken with memoized values
   since the autograd graph may be reversed. For example while the following
   works with or without caching::

       y = t(x)
       t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.

   However the following will error when caching due to dependency reversal::

       y = t(x)
       z = t.inv(y)
       grad(z.sum(), [y])  # error because z is x

   Derived classes should implement one or both of :meth:`_call` or
   :meth:`_inverse`. Derived classes that set `bijective=True` should also
   implement :meth:`log_abs_det_jacobian`.

   :param cache_size: Size of cache. If zero, no caching is done. If one,
                      the latest single value is cached. Only 0 and 1 are supported.
   :type cache_size: int

   .. attribute:: domain

      The constraint representing valid inputs to this transform.

      :type: :class:`~torch.distributions.constraints.Constraint`

   .. attribute:: codomain

      The constraint representing valid outputs to this transform
      which are inputs to the inverse transform.

      :type: :class:`~torch.distributions.constraints.Constraint`

   .. attribute:: bijective

      Whether this transform is bijective. A transform
      ``t`` is bijective iff ``t.inv(t(x)) == x`` and
      ``t(t.inv(y)) == y`` for every ``x`` in the domain and ``y`` in
      the codomain. Transforms that are not bijective should at least
      maintain the weaker pseudoinverse properties
      ``t(t.inv(t(x)) == t(x)`` and ``t.inv(t(t.inv(y))) == t.inv(y)``.

      :type: bool

   .. attribute:: sign

      For bijective univariate transforms, this
      should be +1 or -1 depending on whether transform is monotone
      increasing or decreasing.

      :type: int or Tensor

   .. py:attribute:: bijective
      :value: True

      

   .. py:attribute:: domain

      

   .. py:attribute:: codomain

      

   .. py:method:: _call(x)

      Abstract method to compute forward transformation.


   .. py:method:: _inverse(y)

      Abstract method to compute inverse transformation.


   .. py:method:: log_abs_det_jacobian(x, y)

      Computes the log det jacobian `log |dy/dx|` given input and output.



.. py:class:: RandomFlipTransform(cache_size=0)


   Bases: :py:obj:`torch.distributions.Transform`

   Abstract class for invertable transformations with computable log
   det jacobians. They are primarily used in
   :class:`torch.distributions.TransformedDistribution`.

   Caching is useful for transforms whose inverses are either expensive or
   numerically unstable. Note that care must be taken with memoized values
   since the autograd graph may be reversed. For example while the following
   works with or without caching::

       y = t(x)
       t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.

   However the following will error when caching due to dependency reversal::

       y = t(x)
       z = t.inv(y)
       grad(z.sum(), [y])  # error because z is x

   Derived classes should implement one or both of :meth:`_call` or
   :meth:`_inverse`. Derived classes that set `bijective=True` should also
   implement :meth:`log_abs_det_jacobian`.

   :param cache_size: Size of cache. If zero, no caching is done. If one,
                      the latest single value is cached. Only 0 and 1 are supported.
   :type cache_size: int

   .. attribute:: domain

      The constraint representing valid inputs to this transform.

      :type: :class:`~torch.distributions.constraints.Constraint`

   .. attribute:: codomain

      The constraint representing valid outputs to this transform
      which are inputs to the inverse transform.

      :type: :class:`~torch.distributions.constraints.Constraint`

   .. attribute:: bijective

      Whether this transform is bijective. A transform
      ``t`` is bijective iff ``t.inv(t(x)) == x`` and
      ``t(t.inv(y)) == y`` for every ``x`` in the domain and ``y`` in
      the codomain. Transforms that are not bijective should at least
      maintain the weaker pseudoinverse properties
      ``t(t.inv(t(x)) == t(x)`` and ``t.inv(t(t.inv(y))) == t.inv(y)``.

      :type: bool

   .. attribute:: sign

      For bijective univariate transforms, this
      should be +1 or -1 depending on whether transform is monotone
      increasing or decreasing.

      :type: int or Tensor

   .. py:attribute:: bijective
      :value: False

      

   .. py:attribute:: domain

      

   .. py:attribute:: codomain

      

   .. py:method:: _call(x)

      Abstract method to compute forward transformation.



.. py:class:: CubicCorrelatedNormal(rho: float)


   Bases: :py:obj:`pyro.distributions.ConditionalDistribution`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:method:: condition(context)

      :rtype: torch.distributions.Distribution



.. py:class:: Translate(b)


   Bases: :py:obj:`torch.distributions.Transform`

   Abstract class for invertable transformations with computable log
   det jacobians. They are primarily used in
   :class:`torch.distributions.TransformedDistribution`.

   Caching is useful for transforms whose inverses are either expensive or
   numerically unstable. Note that care must be taken with memoized values
   since the autograd graph may be reversed. For example while the following
   works with or without caching::

       y = t(x)
       t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.

   However the following will error when caching due to dependency reversal::

       y = t(x)
       z = t.inv(y)
       grad(z.sum(), [y])  # error because z is x

   Derived classes should implement one or both of :meth:`_call` or
   :meth:`_inverse`. Derived classes that set `bijective=True` should also
   implement :meth:`log_abs_det_jacobian`.

   :param cache_size: Size of cache. If zero, no caching is done. If one,
                      the latest single value is cached. Only 0 and 1 are supported.
   :type cache_size: int

   .. attribute:: domain

      The constraint representing valid inputs to this transform.

      :type: :class:`~torch.distributions.constraints.Constraint`

   .. attribute:: codomain

      The constraint representing valid outputs to this transform
      which are inputs to the inverse transform.

      :type: :class:`~torch.distributions.constraints.Constraint`

   .. attribute:: bijective

      Whether this transform is bijective. A transform
      ``t`` is bijective iff ``t.inv(t(x)) == x`` and
      ``t(t.inv(y)) == y`` for every ``x`` in the domain and ``y`` in
      the codomain. Transforms that are not bijective should at least
      maintain the weaker pseudoinverse properties
      ``t(t.inv(t(x)) == t(x)`` and ``t.inv(t(t.inv(y))) == t.inv(y)``.

      :type: bool

   .. attribute:: sign

      For bijective univariate transforms, this
      should be +1 or -1 depending on whether transform is monotone
      increasing or decreasing.

      :type: int or Tensor

   .. py:attribute:: bijective
      :value: True

      

   .. py:attribute:: domain

      

   .. py:attribute:: codomain

      

   .. py:method:: _call(x)

      Abstract method to compute forward transformation.


   .. py:method:: _inverse(y)

      Abstract method to compute inverse transformation.


   .. py:method:: log_abs_det_jacobian(x, y)

      Computes the log det jacobian `log |dy/dx|` given input and output.



.. py:class:: SkewedCorrelatedNormal(rho: float)


   Bases: :py:obj:`pyro.distributions.ConditionalDistribution`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:method:: condition(context)

      :rtype: torch.distributions.Distribution



