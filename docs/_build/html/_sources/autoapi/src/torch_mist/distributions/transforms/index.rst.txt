:py:mod:`src.torch_mist.distributions.transforms`
=================================================

.. py:module:: src.torch_mist.distributions.transforms


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   base/index.rst
   linear/index.rst
   permute/index.rst
   split/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   src.torch_mist.distributions.transforms.DistributionModule
   src.torch_mist.distributions.transforms.ConditionalDistributionModule
   src.torch_mist.distributions.transforms.TransformedDistributionModule
   src.torch_mist.distributions.transforms.ConditionalTransformedDistributionModule
   src.torch_mist.distributions.transforms.Linear
   src.torch_mist.distributions.transforms.ConditionalLinear
   src.torch_mist.distributions.transforms.SplitTransform
   src.torch_mist.distributions.transforms.Permute



Functions
~~~~~~~~~

.. autoapisummary::

   src.torch_mist.distributions.transforms.linear
   src.torch_mist.distributions.transforms.conditional_linear
   src.torch_mist.distributions.transforms.conditional_skip_linear
   src.torch_mist.distributions.transforms.permute



.. py:class:: DistributionModule(validate_args: bool = True)


   Bases: :py:obj:`torch.distributions.Distribution`, :py:obj:`torch.nn.Module`, :py:obj:`abc.ABC`

   Distribution is the abstract base class for probability distributions.

   .. py:method:: __repr__()

      Return repr(self).



.. py:class:: ConditionalDistributionModule


   Bases: :py:obj:`pyro.distributions.ConditionalDistribution`, :py:obj:`torch.nn.Module`, :py:obj:`abc.ABC`

   Helper class that provides a standard way to create an ABC using
   inheritance.


.. py:class:: TransformedDistributionModule(base_dist: torch.distributions.Distribution, transforms: Union[torch.distributions.Transform, List[torch.distributions.Transform], Dict[str, torch.distributions.Transform], None])


   Bases: :py:obj:`DistributionModule`, :py:obj:`abc.ABC`

   Distribution is the abstract base class for probability distributions.

   .. py:method:: rsample(sample_shape=torch.Size())

      Generates a sample_shape shaped reparameterized sample or sample_shape
      shaped batch of reparameterized samples if the distribution parameters
      are batched.


   .. py:method:: log_prob(value)

      Returns the log of the probability density/mass function evaluated at
      `value`.

      :param value:
      :type value: Tensor


   .. py:method:: __repr__()

      Return repr(self).



.. py:class:: ConditionalTransformedDistributionModule(base_dist: Union[pyro.distributions.ConditionalDistribution, torch.distributions.Distribution], transforms: Union[pyro.distributions.ConditionalTransform, List[pyro.distributions.ConditionalTransform], Dict[str, Union[pyro.distributions.ConditionalTransform, torch.distributions.Transform]], torch.distributions.Transform, List[torch.distributions.Transform], None])


   Bases: :py:obj:`pyro.distributions.ConditionalTransformedDistribution`, :py:obj:`torch.nn.Module`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:method:: condition(context)

      :rtype: torch.distributions.Distribution


   .. py:method:: clear_cache()



.. py:class:: Linear(input_dim, loc=None, scale=None, initial_scale=None, epsilon=1e-06)


   Bases: :py:obj:`ConditionedLinear`, :py:obj:`pyro.distributions.TransformModule`

   Abstract class for invertable transformations with computable log
   det jacobians. They are primarily used in
   :class:`torch.distributions.TransformedDistribution`.

   Caching is useful for transforms whose inverses are either expensive or
   numerically unstable. Note that care must be taken with memoized values
   since the autograd graph may be reversed. For example while the following
   works with or without caching::

       y = t(x)
       t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.

   However the following will error when caching due to dependency reversal::

       y = t(x)
       z = t.inv(y)
       grad(z.sum(), [y])  # error because z is x

   Derived classes should implement one or both of :meth:`_call` or
   :meth:`_inverse`. Derived classes that set `bijective=True` should also
   implement :meth:`log_abs_det_jacobian`.

   :param cache_size: Size of cache. If zero, no caching is done. If one,
                      the latest single value is cached. Only 0 and 1 are supported.
   :type cache_size: int

   .. attribute:: domain

      The constraint representing valid inputs to this transform.

      :type: :class:`~torch.distributions.constraints.Constraint`

   .. attribute:: codomain

      The constraint representing valid outputs to this transform
      which are inputs to the inverse transform.

      :type: :class:`~torch.distributions.constraints.Constraint`

   .. attribute:: bijective

      Whether this transform is bijective. A transform
      ``t`` is bijective iff ``t.inv(t(x)) == x`` and
      ``t(t.inv(y)) == y`` for every ``x`` in the domain and ``y`` in
      the codomain. Transforms that are not bijective should at least
      maintain the weaker pseudoinverse properties
      ``t(t.inv(t(x)) == t(x)`` and ``t.inv(t(t.inv(y))) == t.inv(y)``.

      :type: bool

   .. attribute:: sign

      For bijective univariate transforms, this
      should be +1 or -1 depending on whether transform is monotone
      increasing or decreasing.

      :type: int or Tensor

   .. py:method:: _params()



.. py:class:: ConditionalLinear(net, loc=None, scale=None, initial_scale=None, epsilon=1e-06, skip_connection=False)


   Bases: :py:obj:`pyro.distributions.ConditionalTransformModule`

   Conditional transforms with learnable parameters such as normalizing flows should inherit from this class rather
   than :class:`~pyro.distributions.conditional.ConditionalTransform` so they are also a subclass of
   :class:`~torch.nn.Module` and inherit all the useful methods of that class.

   .. py:attribute:: domain

      

   .. py:attribute:: codomain

      

   .. py:attribute:: bijective
      :value: True

      

   .. py:method:: _params(context)


   .. py:method:: condition(context)

      :rtype: torch.distributions.Transform



.. py:function:: linear(input_dim, loc=None, scale=None, initial_scale=None)


.. py:function:: conditional_linear(input_dim, context_dim, hidden_dims=None, scale=None, initial_scale=None)


.. py:function:: conditional_skip_linear(input_dim, context_dim, hidden_dims=None, learn_loc=False, scale=None, initial_scale: float = 1.0)


.. py:class:: SplitTransform(input_dim, out_dim)


   Bases: :py:obj:`pyro.distributions.TransformModule`

   Transforms with learnable parameters such as normalizing flows should inherit from this class rather
   than `Transform` so they are also a subclass of `nn.Module` and inherit all the useful methods of that class.

   .. py:attribute:: domain

      

   .. py:attribute:: codomain

      

   .. py:method:: dist()


   .. py:method:: add_dims(x)


   .. py:method:: remove_dims(y)


   .. py:method:: _call(x)

      Abstract method to compute forward transformation.


   .. py:method:: _inverse(y)

      Abstract method to compute inverse transformation.


   .. py:method:: log_abs_det_jacobian(x, y)

      Computes the log det jacobian `log |dy/dx|` given input and output.



.. py:class:: Permute(permutation, *, dim=-1, cache_size=1)


   Bases: :py:obj:`pyro.distributions.transforms.Permute`, :py:obj:`pyro.distributions.TransformModule`

   A bijection that reorders the input dimensions, that is, multiplies the input by
   a permutation matrix. This is useful in between
   :class:`~pyro.distributions.transforms.AffineAutoregressive` transforms to
   increase the flexibility of the resulting distribution and stabilize learning.
   Whilst not being an autoregressive transform, the log absolute determinate of
   the Jacobian is easily calculable as 0. Note that reordering the input dimension
   between two layers of
   :class:`~pyro.distributions.transforms.AffineAutoregressive` is not equivalent
   to reordering the dimension inside the MADE networks that those IAFs use; using
   a :class:`~pyro.distributions.transforms.Permute` transform results in a
   distribution with more flexibility.

   Example usage:

   >>> from pyro.nn import AutoRegressiveNN
   >>> from pyro.distributions.transforms import AffineAutoregressive, Permute
   >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))
   >>> iaf1 = AffineAutoregressive(AutoRegressiveNN(10, [40]))
   >>> ff = Permute(torch.randperm(10, dtype=torch.long))
   >>> iaf2 = AffineAutoregressive(AutoRegressiveNN(10, [40]))
   >>> flow_dist = dist.TransformedDistribution(base_dist, [iaf1, ff, iaf2])
   >>> flow_dist.sample()  # doctest: +SKIP

   :param permutation: a permutation ordering that is applied to the inputs.
   :type permutation: torch.LongTensor
   :param dim: the tensor dimension to permute. This value must be negative and
       defines the event dim as `abs(dim)`.
   :type dim: int


   .. py:method:: update_device()


   .. py:method:: _call(x)

      :param x: the input into the bijection
      :type x: torch.Tensor

      Invokes the bijection x=>y; in the prototypical context of a
      :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
      the base distribution (or the output of a previous transform)


   .. py:method:: _inverse(y)

      :param y: the output of the bijection
      :type y: torch.Tensor

      Inverts y => x.


   .. py:method:: log_abs_det_jacobian(x, y)

      Calculates the elementwise determinant of the log Jacobian, i.e.
      log(abs([dy_0/dx_0, ..., dy_{N-1}/dx_{N-1}])). Note that this type of
      transform is not autoregressive, so the log Jacobian is not the sum of the
      previous expression. However, it turns out it's always 0 (since the
      determinant is -1 or +1), and so returning a vector of zeros works.



.. py:function:: permute(input_dim, permutation=None, dim=-1)

   A helper function to create a :class:`~pyro.distributions.transforms.Permute`
   object for consistency with other helpers.

   :param input_dim: Dimension(s) of input variable to permute. Note that when
       `dim < -1` this must be a tuple corresponding to the event shape.
   :type input_dim: int
   :param permutation: Torch tensor of integer indices representing permutation.
       Defaults to a random permutation.
   :type permutation: torch.LongTensor
   :param dim: the tensor dimension to permute. This value must be negative and
       defines the event dim as `abs(dim)`.
   :type dim: int



