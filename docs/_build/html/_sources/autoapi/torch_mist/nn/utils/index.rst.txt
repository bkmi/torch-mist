:py:mod:`torch_mist.nn.utils`
=============================

.. py:module:: torch_mist.nn.utils


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   torch_mist.nn.utils.JointDenseNN
   torch_mist.nn.utils.Constant
   torch_mist.nn.utils.Identity
   torch_mist.nn.utils.SkipDenseNN
   torch_mist.nn.utils.MergeOutputs




.. py:class:: JointDenseNN(input_dims, hidden_dims, param_dims, dim=-1)


   Bases: :py:obj:`pyro.nn.DenseNN`

   An implementation of a simple dense feedforward network, for use in, e.g., some conditional flows such as
   :class:`pyro.distributions.transforms.ConditionalPlanarFlow` and other unconditional flows such as
   :class:`pyro.distributions.transforms.AffineCoupling` that do not require an autoregressive network.

   Example usage:

   >>> input_dim = 10
   >>> context_dim = 5
   >>> z = torch.rand(100, context_dim)
   >>> nn = DenseNN(context_dim, [50], param_dims=[1, input_dim, input_dim])
   >>> a, b, c = nn(z)  # parameters of size (100, 1), (100, 10), (100, 10)

   :param input_dim: the dimensionality of the input
   :type input_dim: int
   :param hidden_dims: the dimensionality of the hidden units per layer
   :type hidden_dims: list[int]
   :param param_dims: shape the output into parameters of dimension (p_n,) for p_n in param_dims
       when p_n > 1 and dimension () when p_n == 1. The default is [1, 1], i.e. output two parameters of dimension ().
   :type param_dims: list[int]
   :param nonlinearity: The nonlinearity to use in the feedforward network such as torch.nn.ReLU(). Note that no
       nonlinearity is applied to the final network output, so the output is an unbounded real number.
   :type nonlinearity: torch.nn.module


   .. py:method:: forward(*x)



.. py:class:: Constant(value: torch.Tensor)


   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward(*args, **kwargs)


   .. py:method:: __repr__()

      Return repr(self).



.. py:class:: Identity(*args, **kwargs)


   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward(x)



.. py:class:: SkipDenseNN(input_dim, hidden_dims, param_dims=[1, 1], nonlinearity=torch.nn.ReLU())


   Bases: :py:obj:`pyro.nn.DenseNN`

   An implementation of a simple dense feedforward network, for use in, e.g., some conditional flows such as
   :class:`pyro.distributions.transforms.ConditionalPlanarFlow` and other unconditional flows such as
   :class:`pyro.distributions.transforms.AffineCoupling` that do not require an autoregressive network.

   Example usage:

   >>> input_dim = 10
   >>> context_dim = 5
   >>> z = torch.rand(100, context_dim)
   >>> nn = DenseNN(context_dim, [50], param_dims=[1, input_dim, input_dim])
   >>> a, b, c = nn(z)  # parameters of size (100, 1), (100, 10), (100, 10)

   :param input_dim: the dimensionality of the input
   :type input_dim: int
   :param hidden_dims: the dimensionality of the hidden units per layer
   :type hidden_dims: list[int]
   :param param_dims: shape the output into parameters of dimension (p_n,) for p_n in param_dims
       when p_n > 1 and dimension () when p_n == 1. The default is [1, 1], i.e. output two parameters of dimension ().
   :type param_dims: list[int]
   :param nonlinearity: The nonlinearity to use in the feedforward network such as torch.nn.ReLU(). Note that no
       nonlinearity is applied to the final network output, so the output is an unbounded real number.
   :type nonlinearity: torch.nn.module


   .. py:method:: forward(x)



.. py:class:: MergeOutputs(*nets)


   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward(x)



