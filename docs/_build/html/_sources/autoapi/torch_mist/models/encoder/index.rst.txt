:py:mod:`torch_mist.models.encoder`
===================================

.. py:module:: torch_mist.models.encoder


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   mlp/index.rst
   slow_updates/index.rst
   types/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   torch_mist.models.encoder.EncoderKeywords
   torch_mist.models.encoder.EncoderMLP
   torch_mist.models.encoder.SlowlyUpdatingModel




.. py:class:: EncoderKeywords


   Bases: :py:obj:`enum.Enum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: same
      :value: 'same'

      

   .. py:attribute:: slow
      :value: 'slow'

      


.. py:class:: EncoderMLP(input_dim: int, hidden_dims: List[int], output_dim: int)


   Bases: :py:obj:`pyro.nn.DenseNN`

   An implementation of a simple dense feedforward network, for use in, e.g., some conditional flows such as
   :class:`pyro.distributions.transforms.ConditionalPlanarFlow` and other unconditional flows such as
   :class:`pyro.distributions.transforms.AffineCoupling` that do not require an autoregressive network.

   Example usage:

   >>> input_dim = 10
   >>> context_dim = 5
   >>> z = torch.rand(100, context_dim)
   >>> nn = DenseNN(context_dim, [50], param_dims=[1, input_dim, input_dim])
   >>> a, b, c = nn(z)  # parameters of size (100, 1), (100, 10), (100, 10)

   :param input_dim: the dimensionality of the input
   :type input_dim: int
   :param hidden_dims: the dimensionality of the hidden units per layer
   :type hidden_dims: list[int]
   :param param_dims: shape the output into parameters of dimension (p_n,) for p_n in param_dims
       when p_n > 1 and dimension () when p_n == 1. The default is [1, 1], i.e. output two parameters of dimension ().
   :type param_dims: list[int]
   :param nonlinearity: The nonlinearity to use in the feedforward network such as torch.nn.ReLU(). Note that no
       nonlinearity is applied to the final network output, so the output is an unbounded real number.
   :type nonlinearity: torch.nn.module



.. py:class:: SlowlyUpdatingModel(model: torch.nn.Module, tau: float)


   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:property:: current_model
      :type: torch.nn.Module


   .. py:method:: _update_weights()

      Update target network parameters.


   .. py:method:: forward(x: torch.Tensor) -> torch.Tensor



