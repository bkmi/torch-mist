data:
  class_path: core.data.datamodule.MultiOmniglotDataModule
  init_args:
    n_images: 1
    num_workers: 24
    batch_size: 256

model:
  class_path: core.task.InfoMax
  init_args:
    encoder_x:
      class_path: core.models.encoder.OmniglotResNet
      init_args:
        n_images: 1
        out_dim: 64
    encoder_y: same
    mi_estimator:
      class_path: core.models.mi_estimator.HybridMutualInformationEstimator
      init_args:
        generative_estimator:
          class_path: core.models.mi_estimator.ABC
          init_args:
            # Using a Normal distribution with learnable (conditional) mean and variance
            conditional_a_y:
              class_path: core.models.learnable_distributions.ConditionalCategoricalMLP
              init_args:
                y_dim: 64
                n_classes: 128 # Same as the number of clusters
                hidden_dims: [ 128 ]
            marginal_a:
              class_path: core.models.learnable_distributions.LearnableCategorical
              init_args:
                n_classes: 128 # Same as the number of clusters
    optimizer_class: torch.optim.Adam
    optimizer_init_args:
      lr: 1e-4
    lr_scheduler_class: core.optim.lr_scheduler.WarmupScheduler
    lr_scheduler_init_args:
      warmup_steps: 20000
      total_steps: 200000
    lr_scheduler_params:
      frequency: 1
      interval: step

trainer:
  max_steps: 100000
  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
    - class_path: core.callbacks.VarianceLogCallback
      init_args:
          key: ["mi/value", "mi/grad", "mi/gen/value", "mi/gen/grad", "mi/dis/value", "mi/dis/grad"]
    - class_path: core.callbacks.ClusteringCallback
      init_args:
        steps: 100000
        key_to_encode: 'x'
        clustering:
          class_path: sklearn.cluster.KMeans
          init_args:
            n_clusters: 128
            n_init: 50